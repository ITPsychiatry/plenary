{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn import preprocessing\n",
    "import joblib\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import skfuzzy as fuzz\n",
    "import seaborn as sns\n",
    "import plotly as py\n",
    "import plotly.graph_objs as go\n",
    "import os\n",
    "from copy import deepcopy\n",
    "import warnings\n",
    "import ls_functions as lsf\n",
    "import shap\n",
    "import scipy.stats as stats\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataStreamPath = os.getcwd() + \"\\\\\"\n",
    "graphsStreamPath= os.getcwd() + \"\\\\graphs\\\\\"\n",
    "shapvaluesStreamPath= os.getcwd() + \"\\\\shapvalues\\\\\"\n",
    "oneheadmodelsStreamPath= os.getcwd() + \"\\\\onehead_models\\\\\"\n",
    "baselinemodelsStreamPath= os.getcwd() + \"\\\\baseline_models\\\\\"\n",
    "protoformsStreamPath= os.getcwd() + \"\\\\protoforms_small\\\\\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"df_train_noise.csv\")\n",
    "df_test = pd.read_csv(\"df_test_noise.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRAIN SET\n",
    "X_train = df_train.loc[:, 'pcm_LOGenergy_sma':'pcm_fftMag_mfcc_12_']\n",
    "y_train_symptoms = df_train.loc[:, 'anxiety':'suicide']\n",
    "y_train_states = df_train.loc[:, 'hamd_ymrs']\n",
    "\n",
    "# from categorical to numeric target\n",
    "label_coding = {'euthymia' : 0,\n",
    "                'depression' : 1,\n",
    "                'mania' : 2,\n",
    "                'mixed': 3}\n",
    "\n",
    "y_train_states_encoded = np.array(y_train_states.map(label_coding).astype(int))\n",
    "\n",
    "# #TEST SET\n",
    "X_test = df_test.loc[:, 'pcm_LOGenergy_sma':'pcm_fftMag_mfcc_12_']\n",
    "y_test_symptoms = df_test.loc[:, 'anxiety':'suicide']\n",
    "y_test_states = df_test.loc[:, 'hamd_ymrs']\n",
    "\n",
    "y_test_states_encoded = np.array(y_test_states.map(label_coding).astype(int))\n",
    "\n",
    "# # standardize data\n",
    "scaler = preprocessing.StandardScaler()\n",
    "scaler.fit(X_train.values)\n",
    "X_train_scaled = scaler.transform(X_train.values)\n",
    "X_test_scaled = scaler.transform(X_test.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:45:10] WARNING: ..\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "classes_names_states= list(label_coding.keys())\n",
    "\n",
    "feature_names=X_test.columns\n",
    "\n",
    "X_train_scale_df = pd.DataFrame(X_train_scaled, columns = feature_names)\n",
    "\n",
    "#Train the XGBoost model\n",
    "#We create a dictionary that contains our model hyperparameters\n",
    "xgb_params = {\n",
    "    'n_estimators': 500, \n",
    "    #'learning_rate': 0.1,\n",
    "    #'subsample': 0.8,\n",
    "    #'reg_alpha': 1,\n",
    "    'max_depth': 3, #it was 10\n",
    "    'objective': 'multi:softprob', #'binary:logistic',\n",
    "    'num_class': 4\n",
    "    #'scale_pos_weight': 5\n",
    "}\n",
    "xgb_model = XGBClassifier(**xgb_params,use_label_encoder =False)\n",
    "xgb_model = xgb_model.fit(X_train_scale_df, y_train_states_encoded) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 186  335   87   67]\n",
      " [ 593 3002  548  221]\n",
      " [  55  294   66   17]\n",
      " [  89  231   68   78]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.20      0.28      0.23       675\n",
      "           1       0.78      0.69      0.73      4364\n",
      "           2       0.09      0.15      0.11       432\n",
      "           3       0.20      0.17      0.18       466\n",
      "\n",
      "    accuracy                           0.56      5937\n",
      "   macro avg       0.32      0.32      0.31      5937\n",
      "weighted avg       0.62      0.56      0.59      5937\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_xgb=xgb_model.predict(X_test)\n",
    "xgb_cm = confusion_matrix(y_test_states_encoded, y_pred_xgb, labels=xgb_model.classes_)\n",
    "xgb_cr = classification_report(y_test_states_encoded, y_pred_xgb)\n",
    "\n",
    "\n",
    "print(xgb_cm)\n",
    "print(xgb_cr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Baseline approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.a) Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 64)                5568      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " output (Dense)              (None, 4)                 260       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,828\n",
      "Trainable params: 5,828\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_model():\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Dense(64, input_shape=(86,), activation='relu', name='dense'),\n",
    "        tf.keras.layers.Dropout(0.2, name='dropout'),\n",
    "        tf.keras.layers.Dense(4, activation='softmax', name='output')])\n",
    "    model.build()\n",
    "    return model\n",
    "\n",
    "model_name = \"baseline\"\n",
    "baseline = build_model()\n",
    "\n",
    "baseline.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "557/557 [==============================] - 1s 2ms/step - loss: 0.8347 - accuracy: 0.7441 - val_loss: 0.8593 - val_accuracy: 0.7337\n",
      "Epoch 2/15\n",
      "557/557 [==============================] - 1s 1ms/step - loss: 0.7077 - accuracy: 0.7743 - val_loss: 0.8608 - val_accuracy: 0.7379\n",
      "Epoch 3/15\n",
      "557/557 [==============================] - 1s 1ms/step - loss: 0.6744 - accuracy: 0.7824 - val_loss: 0.8368 - val_accuracy: 0.7319\n",
      "Epoch 4/15\n",
      "557/557 [==============================] - 1s 1ms/step - loss: 0.6544 - accuracy: 0.7854 - val_loss: 0.9147 - val_accuracy: 0.6953\n",
      "Epoch 5/15\n",
      "557/557 [==============================] - 1s 1ms/step - loss: 0.6477 - accuracy: 0.7866 - val_loss: 1.0685 - val_accuracy: 0.6439\n"
     ]
    }
   ],
   "source": [
    "baseline.compile(optimizer='adam',\n",
    "                loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False), \n",
    "                metrics=['accuracy'])\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=3) \n",
    "\n",
    "baseline.fit(X_train_scaled, y_train_states_encoded, epochs=15,\n",
    "            validation_data=(X_test_scaled, y_test_states_encoded),\n",
    "            callbacks=[early_stopping])\n",
    "\n",
    "y_pred_states = baseline.predict(X_test_scaled)\n",
    "y_pred_states = np.argmax(y_pred_states, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 284  269    5  117]\n",
      " [ 629 3295   84  356]\n",
      " [  73  302   11   46]\n",
      " [  58  174    1  233]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.27      0.42      0.33       675\n",
      "           1       0.82      0.76      0.78      4364\n",
      "           2       0.11      0.03      0.04       432\n",
      "           3       0.31      0.50      0.38       466\n",
      "\n",
      "    accuracy                           0.64      5937\n",
      "   macro avg       0.38      0.43      0.38      5937\n",
      "weighted avg       0.66      0.64      0.65      5937\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cm_base = confusion_matrix(y_test_states_encoded, y_pred_states)\n",
    "cr_base = classification_report(y_test_states_encoded, y_pred_states)\n",
    "\n",
    "print(cm_base)\n",
    "print(cr_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.b) SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare data for shap\n",
    "X_train_summary = shap.sample(X_train_scaled, 100)\n",
    "end = len(X_test_scaled)\n",
    "feature_names=X_test.columns \n",
    "classes_names_states= list(label_coding.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [05:08<00:00,  5.14s/it]\n"
     ]
    }
   ],
   "source": [
    "#calculate shap values\n",
    "explainer = shap.KernelExplainer(baseline.predict, X_train_summary) \n",
    "shap_values = explainer.shap_values(X_test_scaled[1:end:100, : ]) \n",
    "data_shap_base = pd.DataFrame(X_test_scaled[1:end:100,:], columns = feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 576x2584.8 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# PLOTS\n",
    "for max_features in [20, 40, 86]: \n",
    "  shap.summary_plot(shap_values, X_test_scaled[1:end:10,:], plot_type=\"bar\", class_names= classes_names_states,\n",
    "                    feature_names = feature_names, max_display=max_features , show=False)\n",
    "  plt.gcf()\n",
    "  figname=graphsStreamPath+model_name+'_global_allclasses_states_'+str(max_features)+'.png'\n",
    "  plt.savefig(figname,dpi=150, bbox_inches='tight')\n",
    "  plt.clf()\n",
    "\n",
    "#I'm plotting the global explanations for all the classes, varying the number of features to show\n",
    "#I'm iterating on the number of classes (numerical)\n",
    "for class_id in range(len(shap_values)):\n",
    "  #I'm iterating on the number of features I want to plot\n",
    "  for max_features in [20, 40, 86]: \n",
    "    shap.summary_plot(shap_values[class_id], X_test_scaled[1:end:100,:], feature_names = feature_names,\n",
    "                      max_display=max_features,show=False)\n",
    "    plt.gcf()\n",
    "    figname=graphsStreamPath+model_name+'_global_class'+str(class_id)+'_features'+ str(max_features)+'.png'\n",
    "    plt.savefig(figname,dpi=150, bbox_inches='tight')\n",
    "    plt.clf()\n",
    "\n",
    "shap_values_0_class_base = pd.DataFrame(shap_values[0], columns = feature_names)\n",
    "shap_values_1_class_base = pd.DataFrame(shap_values[1], columns = feature_names)\n",
    "shap_values_2_class_base = pd.DataFrame(shap_values[2], columns = feature_names)\n",
    "shap_values_3_class_base = pd.DataFrame(shap_values[3], columns = feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Compositional MLP approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.a) Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"one-head-model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input (InputLayer)          [(None, 86)]              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                5568      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " symptom_output (Dense)      (None, 10)                650       \n",
      "                                                                 \n",
      " state_output (Dense)        (None, 4)                 44        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,262\n",
      "Trainable params: 6,262\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/15\n",
      "557/557 [==============================] - 2s 2ms/step - loss: 0.9941 - symptom_output_loss: 1.1548 - state_output_loss: 0.8333 - symptom_output_mae: 1.1548 - symptom_output_accuracy: 0.1587 - state_output_mae: 0.9346 - state_output_accuracy: 0.7416 - val_loss: 1.0063 - val_symptom_output_loss: 1.1542 - val_state_output_loss: 0.8584 - val_symptom_output_mae: 1.1542 - val_symptom_output_accuracy: 0.1942 - val_state_output_mae: 0.9229 - val_state_output_accuracy: 0.7445\n",
      "Epoch 2/15\n",
      "557/557 [==============================] - 1s 2ms/step - loss: 0.8598 - symptom_output_loss: 0.9983 - state_output_loss: 0.7213 - symptom_output_mae: 0.9983 - symptom_output_accuracy: 0.2034 - state_output_mae: 0.9346 - state_output_accuracy: 0.7674 - val_loss: 1.0171 - val_symptom_output_loss: 1.1156 - val_state_output_loss: 0.9185 - val_symptom_output_mae: 1.1156 - val_symptom_output_accuracy: 0.1918 - val_state_output_mae: 0.9229 - val_state_output_accuracy: 0.6911\n",
      "Epoch 3/15\n",
      "557/557 [==============================] - 1s 2ms/step - loss: 0.8244 - symptom_output_loss: 0.9549 - state_output_loss: 0.6938 - symptom_output_mae: 0.9549 - symptom_output_accuracy: 0.2057 - state_output_mae: 0.9346 - state_output_accuracy: 0.7752 - val_loss: 1.2008 - val_symptom_output_loss: 1.1652 - val_state_output_loss: 1.2364 - val_symptom_output_mae: 1.1652 - val_symptom_output_accuracy: 0.1693 - val_state_output_mae: 0.9229 - val_state_output_accuracy: 0.5830\n",
      "Epoch 4/15\n",
      "557/557 [==============================] - 1s 2ms/step - loss: 0.8025 - symptom_output_loss: 0.9313 - state_output_loss: 0.6736 - symptom_output_mae: 0.9313 - symptom_output_accuracy: 0.2056 - state_output_mae: 0.9346 - state_output_accuracy: 0.7801 - val_loss: 1.4468 - val_symptom_output_loss: 1.2124 - val_state_output_loss: 1.6813 - val_symptom_output_mae: 1.2124 - val_symptom_output_accuracy: 0.1644 - val_state_output_mae: 0.9229 - val_state_output_accuracy: 0.5274\n",
      "Epoch 5/15\n",
      "557/557 [==============================] - 1s 2ms/step - loss: 0.7898 - symptom_output_loss: 0.9144 - state_output_loss: 0.6653 - symptom_output_mae: 0.9144 - symptom_output_accuracy: 0.2091 - state_output_mae: 0.9346 - state_output_accuracy: 0.7816 - val_loss: 1.6246 - val_symptom_output_loss: 1.2432 - val_state_output_loss: 2.0061 - val_symptom_output_mae: 1.2432 - val_symptom_output_accuracy: 0.1499 - val_state_output_mae: 0.9229 - val_state_output_accuracy: 0.4681\n",
      "Epoch 6/15\n",
      "557/557 [==============================] - 1s 2ms/step - loss: 0.7831 - symptom_output_loss: 0.9079 - state_output_loss: 0.6582 - symptom_output_mae: 0.9079 - symptom_output_accuracy: 0.2103 - state_output_mae: 0.9346 - state_output_accuracy: 0.7857 - val_loss: 1.7971 - val_symptom_output_loss: 1.3077 - val_state_output_loss: 2.2865 - val_symptom_output_mae: 1.3077 - val_symptom_output_accuracy: 0.1443 - val_state_output_mae: 0.9229 - val_state_output_accuracy: 0.4684\n",
      "Epoch 7/15\n",
      "557/557 [==============================] - 1s 2ms/step - loss: 0.7733 - symptom_output_loss: 0.8977 - state_output_loss: 0.6489 - symptom_output_mae: 0.8977 - symptom_output_accuracy: 0.2092 - state_output_mae: 0.9346 - state_output_accuracy: 0.7862 - val_loss: 1.8287 - val_symptom_output_loss: 1.2990 - val_state_output_loss: 2.3584 - val_symptom_output_mae: 1.2990 - val_symptom_output_accuracy: 0.1460 - val_state_output_mae: 0.9229 - val_state_output_accuracy: 0.4546\n",
      "Epoch 8/15\n",
      "557/557 [==============================] - 1s 2ms/step - loss: 0.7654 - symptom_output_loss: 0.8905 - state_output_loss: 0.6404 - symptom_output_mae: 0.8905 - symptom_output_accuracy: 0.2121 - state_output_mae: 0.9346 - state_output_accuracy: 0.7888 - val_loss: 2.1780 - val_symptom_output_loss: 1.4078 - val_state_output_loss: 2.9481 - val_symptom_output_mae: 1.4078 - val_symptom_output_accuracy: 0.1401 - val_state_output_mae: 0.9229 - val_state_output_accuracy: 0.4007\n",
      "Epoch 9/15\n",
      "557/557 [==============================] - 1s 2ms/step - loss: 0.7606 - symptom_output_loss: 0.8857 - state_output_loss: 0.6356 - symptom_output_mae: 0.8857 - symptom_output_accuracy: 0.2148 - state_output_mae: 0.9346 - state_output_accuracy: 0.7914 - val_loss: 2.2639 - val_symptom_output_loss: 1.4222 - val_state_output_loss: 3.1056 - val_symptom_output_mae: 1.4222 - val_symptom_output_accuracy: 0.1401 - val_state_output_mae: 0.9229 - val_state_output_accuracy: 0.4108\n",
      "Epoch 10/15\n",
      "557/557 [==============================] - 1s 2ms/step - loss: 0.7563 - symptom_output_loss: 0.8809 - state_output_loss: 0.6316 - symptom_output_mae: 0.8809 - symptom_output_accuracy: 0.2144 - state_output_mae: 0.9346 - state_output_accuracy: 0.7930 - val_loss: 2.5574 - val_symptom_output_loss: 1.5445 - val_state_output_loss: 3.5703 - val_symptom_output_mae: 1.5445 - val_symptom_output_accuracy: 0.1342 - val_state_output_mae: 0.9229 - val_state_output_accuracy: 0.4074\n",
      "Epoch 11/15\n",
      "557/557 [==============================] - 1s 2ms/step - loss: 0.7504 - symptom_output_loss: 0.8771 - state_output_loss: 0.6238 - symptom_output_mae: 0.8771 - symptom_output_accuracy: 0.2156 - state_output_mae: 0.9346 - state_output_accuracy: 0.7967 - val_loss: 2.5368 - val_symptom_output_loss: 1.5036 - val_state_output_loss: 3.5701 - val_symptom_output_mae: 1.5036 - val_symptom_output_accuracy: 0.1368 - val_state_output_mae: 0.9229 - val_state_output_accuracy: 0.3899\n",
      "Epoch 12/15\n",
      "557/557 [==============================] - 1s 2ms/step - loss: 0.7483 - symptom_output_loss: 0.8744 - state_output_loss: 0.6223 - symptom_output_mae: 0.8744 - symptom_output_accuracy: 0.2204 - state_output_mae: 0.9346 - state_output_accuracy: 0.7989 - val_loss: 2.6312 - val_symptom_output_loss: 1.5310 - val_state_output_loss: 3.7314 - val_symptom_output_mae: 1.5310 - val_symptom_output_accuracy: 0.1378 - val_state_output_mae: 0.9229 - val_state_output_accuracy: 0.3823\n",
      "Epoch 13/15\n",
      "557/557 [==============================] - 1s 2ms/step - loss: 0.7450 - symptom_output_loss: 0.8717 - state_output_loss: 0.6184 - symptom_output_mae: 0.8717 - symptom_output_accuracy: 0.2198 - state_output_mae: 0.9346 - state_output_accuracy: 0.8000 - val_loss: 2.8108 - val_symptom_output_loss: 1.5966 - val_state_output_loss: 4.0249 - val_symptom_output_mae: 1.5966 - val_symptom_output_accuracy: 0.1334 - val_state_output_mae: 0.9229 - val_state_output_accuracy: 0.3687\n",
      "Epoch 14/15\n",
      "557/557 [==============================] - 1s 2ms/step - loss: 0.7406 - symptom_output_loss: 0.8677 - state_output_loss: 0.6135 - symptom_output_mae: 0.8677 - symptom_output_accuracy: 0.2193 - state_output_mae: 0.9346 - state_output_accuracy: 0.8008 - val_loss: 2.8630 - val_symptom_output_loss: 1.6025 - val_state_output_loss: 4.1235 - val_symptom_output_mae: 1.6025 - val_symptom_output_accuracy: 0.1347 - val_state_output_mae: 0.9229 - val_state_output_accuracy: 0.3778\n",
      "Epoch 15/15\n",
      "557/557 [==============================] - 1s 2ms/step - loss: 0.7390 - symptom_output_loss: 0.8678 - state_output_loss: 0.6101 - symptom_output_mae: 0.8678 - symptom_output_accuracy: 0.2182 - state_output_mae: 0.9346 - state_output_accuracy: 0.8011 - val_loss: 3.0228 - val_symptom_output_loss: 1.6760 - val_state_output_loss: 4.3696 - val_symptom_output_mae: 1.6760 - val_symptom_output_accuracy: 0.1312 - val_state_output_mae: 0.9229 - val_state_output_accuracy: 0.3903\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x25c81e77100>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model_name='one_head'\n",
    "\n",
    "input = tf.keras.layers.Input(shape=(86,), name='input')\n",
    "hidden = tf.keras.layers.Dense(64, activation='relu', name='dense')(input)\n",
    "dropout = tf.keras.layers.Dropout(0.2, name='dropout')(hidden)\n",
    "symptom_output = tf.keras.layers.Dense(10, name='symptom_output')(dropout)\n",
    "state_output = tf.keras.layers.Dense(4, activation='softmax', name='state_output')(symptom_output)\n",
    "\n",
    "one_head = tf.keras.Model(inputs=input, \n",
    "                          outputs=[symptom_output, state_output], \n",
    "                          name='one-head-model')\n",
    "\n",
    "one_head.summary()\n",
    "\n",
    "one_head.compile(optimizer='adam',\n",
    "                 loss=[tf.keras.losses.MeanAbsoluteError(),\n",
    "                       tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)],\n",
    "                 loss_weights=[0.5, 0.5],\n",
    "                 metrics=['mae', 'accuracy'])\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='state_output_accuracy', patience=3)\n",
    "\n",
    "one_head.fit(X_train_scaled, [y_train_symptoms, y_train_states_encoded], epochs=15, \n",
    "             validation_data=(X_test_scaled, [y_test_symptoms, y_test_states_encoded]),\n",
    "             callbacks=[early_stopping])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 407  164    0  104]\n",
      " [1825 1684    0  855]\n",
      " [ 169  159    0  104]\n",
      " [ 120  120    0  226]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.16      0.60      0.25       675\n",
      "           1       0.79      0.39      0.52      4364\n",
      "           2       0.00      0.00      0.00       432\n",
      "           3       0.18      0.48      0.26       466\n",
      "\n",
      "    accuracy                           0.39      5937\n",
      "   macro avg       0.28      0.37      0.26      5937\n",
      "weighted avg       0.61      0.39      0.43      5937\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n"
     ]
    }
   ],
   "source": [
    "model_name='one_head'\n",
    "y_pred_states = one_head.predict(X_test_scaled)\n",
    "\n",
    "y_pred_states = np.argmax(y_pred_states[1][:], axis=1)\n",
    "\n",
    "cm_oh_class = confusion_matrix(y_test_states_encoded, y_pred_states)\n",
    "cr_oh_class = classification_report(y_test_states_encoded, y_pred_states)\n",
    "\n",
    "print(cm_oh_class)\n",
    "print(cr_oh_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.b) SHAP with states (4 classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare data for shap\n",
    "X_train_summary = shap.sample(X_train_scaled, 100)\n",
    "end = len(X_test_scaled)\n",
    "feature_names=X_test.columns \n",
    "classes_names_states= list(label_coding.keys())\n",
    "\n",
    "data_shap_df=pd.DataFrame(X_test_scaled[1:end:100,:], columns = feature_names)\n",
    "#data_shap_df.to_csv(shapvaluesStreamPath + \"/data_shap.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [06:19<00:00,  6.32s/it]\n"
     ]
    }
   ],
   "source": [
    "def f_states(X):\n",
    "    return one_head.predict(X)[1]# with this function we select the second output of the model: vector of states \n",
    "\n",
    "explainer = shap.KernelExplainer(f_states, X_train_summary)  \n",
    "shap_values = explainer.shap_values(X_test_scaled[1:end:100, : ]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#decoding values of BD stated to BD name\n",
    "classes_names_states = []\n",
    "for label in y_test_states_encoded:\n",
    "     classes_names_states.append(list(label_coding.keys())[list(label_coding.values()).index(label)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 576x2584.8 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#I'm plotting the global explanation for all classes, varying the number of features to share\n",
    "model_name='one_head'\n",
    "for max_features in [20, 40, 86]: \n",
    "  shap.summary_plot(shap_values, X_test_scaled[1:end:100,:], plot_type=\"bar\", class_names= classes_names_states,\n",
    "                    feature_names = feature_names, max_display=max_features , show=False)\n",
    "  plt.gcf()\n",
    "  figname=graphsStreamPath+model_name+'_global_allclasses_states_'+str(max_features)+'.png'\n",
    "  plt.savefig(figname,dpi=150, bbox_inches='tight')\n",
    "  plt.plot()\n",
    "  plt.clf()\n",
    "\n",
    "#I'm plotting the global explanations for all the classes, varying the number of features to show\n",
    "#I'm iterating on the number of classes (numerical)\n",
    "for class_id in range(len(shap_values)):\n",
    "  #I'm iterating on the number of features I want to plot\n",
    "  for max_features in [20, 40, 86]: \n",
    "    shap.summary_plot(shap_values[class_id], X_test_scaled[1:end:100,:], feature_names = feature_names, max_display=max_features,show=False)\n",
    "    plt.gcf()\n",
    "    figname=graphsStreamPath+model_name+'_global_class'+str(class_id)+'_features'+ str(max_features)+'.png'\n",
    "    plt.savefig(figname,dpi=150, bbox_inches='tight')\n",
    "    plt.plot()\n",
    "    plt.clf()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "shap_values_0_class_oh = pd.DataFrame(shap_values[0], columns = feature_names)\n",
    "shap_values_1_class_oh = pd.DataFrame(shap_values[1], columns = feature_names)\n",
    "shap_values_2_class_oh = pd.DataFrame(shap_values[2], columns = feature_names)\n",
    "shap_values_3_class_oh = pd.DataFrame(shap_values[3], columns = feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.c) SHAP with symptoms (10 classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare data for shap\n",
    "X_train_summary = shap.sample(X_train_scaled, 100)\n",
    "end = len(X_test_scaled)\n",
    "feature_names=X_test.columns \n",
    "classes_names_states= list(label_coding.keys())\n",
    "\n",
    "data_shap_oh=pd.DataFrame(X_test_scaled[1:end:100,:], columns = feature_names)\n",
    "data_shap_oh.to_csv(shapvaluesStreamPath + \"/data_shap_onehead.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [06:19<00:00,  6.32s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def f_symptoms(X):\n",
    "    return one_head.predict(X)[0]# with this function we select the second output of the model: vector of states \n",
    "\n",
    "explainer = shap.KernelExplainer(f_symptoms, X_train_summary)\n",
    "shap_values = explainer.shap_values(X_test_scaled[1:end:100,:]) \n",
    "classes_names=y_test_symptoms.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 576x684 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_features=20\n",
    "#I'm plotting the global summary for all the classes, only for 20 features\n",
    "shap.summary_plot(shap_values, X_test_scaled[1:end:100,:], plot_type=\"bar\", \n",
    "                  class_names= classes_names, max_display=max_features, feature_names = feature_names,show=False)\n",
    "plt.gcf()\n",
    "figname=graphsStreamPath+model_name+'_global_allclasses.png'\n",
    "plt.savefig(figname,dpi=150, bbox_inches='tight')\n",
    "plt.clf()\n",
    "\n",
    "for x in range(classes_names.shape[0]):\n",
    "  classes_names[x]\n",
    "  shap.summary_plot(shap_values[x], X_test_scaled[1:end:100,:], feature_names = feature_names,\n",
    "                    max_display=max_features,show=False) #you can change the maximum features to display \n",
    "  plt.gcf()\n",
    "  figname=graphsStreamPath+model_name+'_global_allclasses_'+classes_names[x]+'.png'\n",
    "  plt.savefig(figname,dpi=150, bbox_inches='tight')\n",
    "  plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values_0_symptom_oh = pd.DataFrame(shap_values[0], columns = feature_names)\n",
    "shap_values_1_symptom_oh = pd.DataFrame(shap_values[1], columns = feature_names)\n",
    "shap_values_2_symptom_oh = pd.DataFrame(shap_values[2], columns = feature_names)\n",
    "shap_values_3_symptom_oh = pd.DataFrame(shap_values[3], columns = feature_names)\n",
    "shap_values_4_symptom_oh = pd.DataFrame(shap_values[4], columns = feature_names)\n",
    "shap_values_5_symptom_oh = pd.DataFrame(shap_values[5], columns = feature_names)\n",
    "shap_values_6_symptom_oh = pd.DataFrame(shap_values[6], columns = feature_names)\n",
    "shap_values_7_symptom_oh = pd.DataFrame(shap_values[7], columns = feature_names)\n",
    "shap_values_8_symptom_oh = pd.DataFrame(shap_values[8], columns = feature_names)\n",
    "shap_values_9_symptom_oh = pd.DataFrame(shap_values[9], columns = feature_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "07bc74f91d1f8eb169c55d10374fd0fd42e378e12fcc798a378379f252fa0102"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
